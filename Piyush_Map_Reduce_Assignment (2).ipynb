{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b7u8a0uLQHyY"
   },
   "source": [
    "# ALGORITHMS FOR DATA SCIENCE (by (Prof. Dmitrii Bakhitov)\n",
    "# MapReduce Assignment\n",
    "## CRN:-CS-676 (74060) ; FALL 2023\n",
    "## Submitted by: Piyush Gupta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3MPGjUXIQesf"
   },
   "source": [
    "### Objective:\n",
    "**Apply MapReduce concepts in a real-world data processing scenario using Python.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0yuR8GXgQWrK",
    "outputId": "610c306c-74e5-4b95-fd97-268af6f28f81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n"
     ]
    }
   ],
   "source": [
    "pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "bDNa8XmmQEz2"
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import PyPDF2\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from multiprocessing import Pool\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H5EiX0EMTqt3"
   },
   "source": [
    "PyPDF2 for PDF processing, re for regular expressions, defaultdict from collections for easier data management, Pool from multiprocessing for parallel processing, and time for tracking execution times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ic2agaVQ3kv"
   },
   "source": [
    "### Step 1: Importing the Dataset\n",
    "First, import the necessary libraries and read the content of the PDF file. This will serve as our dataset for the MapReduce task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "c0Jp0DnMQ1VK"
   },
   "outputs": [],
   "source": [
    "pdf_path = 'What Is a Statistical Project.pdf'\n",
    "\n",
    "try:\n",
    "    # Open the PDF file\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        num_pages = len(pdf_reader.pages)\n",
    "\n",
    "        # Extract text from each page and combine it\n",
    "        text_data = \"\"\n",
    "        for page in range(num_pages):\n",
    "            text_data += pdf_reader.pages[page].extract_text() + \"\\n\"\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {pdf_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "id": "wuaKlAB3STbu",
    "outputId": "5d0f6019-76cf-4046-ef9e-717eebc74bce"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'A statistical project is the process of answering \\na research question using statistical techniques and presenting the work in a written report. Th  e \\nresearch question may arise from any fi  eld of \\nscientifi  c endeavor, such as athletics, advertising, \\naerodynamics, or nutrition. A project diff  ers \\nfrom a statistical poster in that a written report is used to present the fi  ndings.\\nData-Based Problemsolving\\nTh e process of developing a statistical project \\nshould demonstrate the scientifi  c method and \\npose a focused question or questions, collect appropriate data, analyze the data thoughtfully, and draw correct conclusions.\\nBecause students are asking questions continually \\nabout what touches their lives, they should have little trouble generating questions about them-selves or their schools, families, neighborhoods, or interesting phenomena in the world.\\nOnce a question is proposed, students should \\nexamine it. First, is it a question that can be answered? (Th  e question “Is there intelligent life \\nin the universe that does not come from Earth?” is an extremely interesting question, but not one that is likely to be answered in a short-term proj-ect.) Second, can students collect data to answer the question or has someone else already collect-ed data that could be used to fi  nd the answer?\\nOnce the question is chosen, data must be \\ncollected. If published data are used, students should understand how the data were obtained and record their source. Usually, students choose to collect their own data. Time should be spent deciding how to collect this data. If a survey is used, how are the people chosen to answer the questionnaire? If two treatments (i.e., models, grades, genders, etc.) are to be compared, how What Is a Statistical Project?\\ncan comparisons be made fairly? How will the data be recorded?\\nAfter the details have been worked out, students \\nare ready to obtain the data. Great care should be exercised at every stage of data collection. Care-less measurement or recording of data cannot be remedied in the analysis phase of a project.\\nTh oughtful analysis of the data may take many \\nforms and should be guided by the question and how the data were collected. Usually, it is best to begin by graphing the data. Can graphs be used to give the answer to the question or questions? Most of the time, graphics have been the sole method of data analysis for grades 4–6. As students gain ex-perience, simple statistical methods such as a chi-squared test or a t-test may be used. Regression \\nhas been used occasionally. Sometimes, estimation is most appropriate and hypothesis testing is not needed. Other methods may be used, depending on the question and data.\\nOnce analysis is complete, the question should be \\nanswered. Th  e data may not be able to provide a \\nconclusive answer. For example, one treatment may appear to be better than another, but the diff  erence \\nwas not signifi  cant. If the question has a defi  nitive \\nanswer, that should be presented. A check should be made at this point to make certain the answer matches the question. It is easy to get caught up in the analysis phase and obtain many answers, none of which addresses the research question.\\nFinally, consider the strengths and weaknesses \\nof the project. What would be changed if the project was done again?\\nTh e Written Report\\nGreat latitude may be taken in developing the \\nwritten report. Students should plan how to \\ncommunicate their work eff  ectively. Th  e longest \\nreport does not necessarily represent the best project. However, the report must accomplish the following:\\n•     Demonstrate how and why the particular  \\n    topic was chosen\\n•     Show how the research was conducted•     Delineate what conclusions were obtained•     Include the collected data and its analysis•     Discuss the strengths and weaknesses of  \\n    the selected statistical methodsA Final Note\\nTh e NCTM Standards for Curriculum and Eval-\\nuation in School Mathematics presents the vision that problemsolving is a main goal of mathemat-ics instruction at all levels and calls for student involvement in statistical activities at all grade levels. Th  e standards indicate statistical think-\\ning should start in the primary grades with the creation of student data from class activities. In upper grades, the emphasis is on collecting, orga-nizing, summarizing, and interpreting data from other school disciplines, such as the physical or social sciences, as well as outside interests of the students. Th  e statistical project is a powerful tool \\nfor attaining these goals while exercising essential communication skills.\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8xy00-L9SWre",
    "outputId": "09d1873d-8464-46e7-c2e2-bc8dd7f3f98a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4632"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yr7sciW0QuvX"
   },
   "source": [
    "### Step 2: Problem Statement\n",
    "\n",
    "Perform a word count analysis on the text data extracted from the PDF. We will count the frequency of each word using the MapReduce model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X0E-uGshQzIU"
   },
   "source": [
    "### Step 3: Implement MapReduce\n",
    "\n",
    "Implement the MapReduce model with custom map and reduce functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "M6ZDZIzmRRqK"
   },
   "outputs": [],
   "source": [
    "# Map Function\n",
    "def map_function(text):\n",
    "    # Split the text into words and map each word to a count of 1\n",
    "    return [(word, 1) for word in re.findall(r'\\w+', text.lower())]\n",
    "\n",
    "# Reduce Function\n",
    "def reduce_function(mapped_items):\n",
    "    # Reduce the mapped items by summing the counts of each word\n",
    "    reduced = defaultdict(int)\n",
    "    for word, count in mapped_items:\n",
    "        reduced[word] += count\n",
    "    return dict(reduced)\n",
    "\n",
    "# Parallel MapReduce Function\n",
    "def parallel_map_reduce(data, map_func, reduce_func, pool_size):\n",
    "    # Use a pool of workers to apply the map function in parallel\n",
    "    with Pool(pool_size) as pool:\n",
    "        map_results = pool.map(map_func, data)\n",
    "        # Flatten the list of lists to a single list of key-value pairs\n",
    "        flattened_map_results = [item for sublist in map_results for item in sublist]\n",
    "        # Reduce the mapped data\n",
    "        reduced_results = reduce_func(flattened_map_results)\n",
    "    return reduced_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ryPN3j80RZki"
   },
   "source": [
    "### Step 4: Performance Analysis\n",
    "Run the MapReduce job and measure the execution time for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "ADMQdX3XRjQV"
   },
   "outputs": [],
   "source": [
    "# Function to run performance analysis with varying pool sizes\n",
    "def performance_analysis(data, map_func, reduce_func, pool_sizes):\n",
    "    times = {}\n",
    "    for size in pool_sizes:\n",
    "        start_time = time.time()\n",
    "        parallel_map_reduce(data, map_func, reduce_func, pool_size=size)\n",
    "        end_time = time.time()\n",
    "        times[size] = end_time - start_time\n",
    "    return times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "POtjFg4HRpfl",
    "outputId": "b3b53f57-78d9-46bd-de8b-cfbdbde93c91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Analysis Times:\n",
      "{1: 0.028107404708862305, 2: 0.027884244918823242, 4: 0.048342227935791016, 8: 0.12561392784118652, 'non_parallel': 0.0005130767822265625}\n"
     ]
    }
   ],
   "source": [
    "#  Proceed with MapReduce and performance analysis\n",
    "if 'text_data' in locals():\n",
    "    # Splitting the text data into chunks for parallel processing\n",
    "    data_chunks = text_data.split('\\n')\n",
    "\n",
    "    # Running the performance analysis with varying pool sizes\n",
    "    pool_sizes = [1, 2, 4, 8]\n",
    "    performance_times = performance_analysis(data_chunks, map_function, reduce_function, pool_sizes)\n",
    "\n",
    "    # Measuring execution time for the non-parallelized approach\n",
    "    start_time_non_parallel = time.time()\n",
    "    word_counts_non_parallel = reduce_function(map_function(text_data))\n",
    "    end_time_non_parallel = time.time()\n",
    "    execution_time_non_parallel = end_time_non_parallel - start_time_non_parallel\n",
    "\n",
    "    # Adding non-parallelized execution time to the performance times dictionary\n",
    "    performance_times['non_parallel'] = execution_time_non_parallel\n",
    "\n",
    "    # Displaying performance times\n",
    "    print(\"Performance Analysis Times:\")\n",
    "    print(performance_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JxVL0uH0R-Rs"
   },
   "source": [
    "### Step 5: Report Writing\n",
    "Document the approach, code, results, and observations. Discuss the scalability and efficiency of the MapReduce model based on the findings, and reflect on the challenges faced and potential improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oe5z0JF8SB3t"
   },
   "source": [
    "### Report Writing for MapReduce Performance Analysis\n",
    "\n",
    "## Approach\n",
    "The approach taken in this MapReduce job involves a parallelized computation to perform a word count on a large text dataset. The code utilizes Python's multiprocessing capabilities to distribute the map and reduce tasks across multiple worker processes. The performance of this parallelized approach is compared with a non-parallelized (sequential) execution. The objective is to understand the impact of process pooling on the execution time of the MapReduce job.\n",
    "\n",
    "## Results\n",
    "The results displayed in the output are as follows:\n",
    "\n",
    "Performance Analysis Times:\n",
    "\n",
    "Performance Analysis Times:\n",
    "{1: 0.028107404708862305, 2: 0.027884244918823242, 4: 0.048342227935791016, 8: 0.12561392784118652, 'non_parallel': 0.0005130767822265625}\n",
    "\n",
    "## Observations\n",
    "From the output, the following observations can be made:\n",
    "\n",
    "- The non-parallelized approach has the fastest execution time at approximately `0.00051` seconds.\n",
    "- With a single process in the pool (which effectively makes it sequential), the execution time is around `0.028` seconds, which is slower than the non-parallelized run.\n",
    "- As the pool size increases to 2, the execution time decreases slightly to `0.0278` seconds, indicating that some benefit is gained from parallel processing.\n",
    "- However, further increasing the pool size to 4 and 8 results in increased execution times, `0.0483` and `0.125` seconds, respectively. This suggests that there is an overhead associated with managing a larger number of processes that outweighs the benefits of parallelism for this task.\n",
    "\n",
    "## Scalability and Efficiency\n",
    "The MapReduce model typically exhibits improved performance with increased parallelism, but the results indicate an inverse relationship between pool size and performance in this specific case. This could be due to several factors such as overhead from process creation and management, the overhead of inter-process communication, or the small size of the dataset which does not require extensive parallel processing.\n",
    "\n",
    "Efficiency in parallel computing is not solely about increasing worker count; it's also about the overhead and the nature of the task. The results suggest that for smaller datasets, or tasks with a quick execution time, the overhead of parallelism can outweigh its benefits.\n",
    "\n",
    "## Challenges Faced\n",
    "The challenges that might have been faced include:\n",
    "\n",
    "- Determining the optimal number of processes for the pool.\n",
    "- Managing the overhead associated with parallel processing.\n",
    "- Ensuring that the task is suitable for parallel execution and that the dataset is large enough to benefit from it.\n",
    "\n",
    "## Potential Improvements\n",
    "To improve the performance of the MapReduce job, one could:\n",
    "\n",
    "- Conduct thorough profiling to understand where the overhead is coming from.\n",
    "- Experiment with different chunk sizes for the dataset to optimize the amount of data processed per worker.\n",
    "- Implement a more sophisticated mechanism for dynamically adjusting the pool size based on the complexity of the task and the size of the dataset.\n",
    "\n",
    "In conclusion, while the MapReduce model is powerful for processing large datasets, it requires careful tuning and consideration of overheads to achieve optimal performance. This case study highlights the importance of understanding the computational environment and the characteristics of the task at hand when designing parallelized solutions.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
